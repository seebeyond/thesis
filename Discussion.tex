\chapter{Discussion}\label{chapter:discussion}

In recent years, online reviews have increasingly become a very important resource for consumers when making purchases. The incentive for companies to try to produce fake reviews to boost sales is also growing. It is becoming more and more difficult for people to make well-informed buying decisions without being deceived by the spammers. 

The opinion spam problem was formulated for the first time only a few years ago, but it has quickly become a captivating research area due to the abundance of user-generated reviews which are increasingly having a bigger impact on online purchases. 

In this thesis, I focused on the problem of detecting opinion spam and proposed two methods which were not previously used in this problem context. In chapters \ref{chapter:introduction}, \ref{chapter:goal} and \ref{chapter:relatedwork}, I defined and motivated the research problem and reviewed the existing approaches in the literature. In section \ref{section:singleton-detection} I proposed a detection method based on semantic similarity, which uses WordNet to compute the relatedness between words. Variants of the cosine similarity were also introduced, as well as a new similarity measure \textit{maxsim}, aimed to give the best results from both vectorial and semantic measures. Experimental results showed that semantic similarity can outperform the vectorial model in detecting deceptive reviews, capturing even more subtle textual clues. The precision score of the review classifier showed high results, enough to make the method viable to be integrated in a production detection system.

In such a system, a critical requirement would be how fast it can analyze all incoming reviews for possible similarity against existing reviews. It is practically unfeasible to compare the text of any two opinions for a seller which has thousands of reviews. As a novel contribution, I have proposed to use density-based clustering to group reviews using simple behavioral features, proven to be linked to suspicious user activity. I have made a comparison between the quality of the clusters obtained using two algorithms - DBSCAN and OPTICS - and measured the impact tuning different input parameters has on the overall classifier results, for this problem. I have argued the noise generated by the clustering is not negligible and should be minimized as much as possible by using more well-crafted behavioral features.

In section \ref{section:distribution-reviews} I compared the distributions of truthful and deceptive reviews in both Trustpilot and Ott datasets. The first contains only real-life reviews while Ott's dataset contains real-life truthful reviews from TripAdvisor and crowdsourced deceptive reviews. It appears that in both datasets, the gap between the distributional curves of the two review types is not so prominent for the cosine-based measures as it is for the semantic one. This supports the hypothesis that it is harder to differentiate between the truthful and the deceptive reviews using vectorial measures than by using the semantic approach. Another interesting aspect observed in the Trustpilot dataset is the steep initial jump of the distributional curves for the vectorial measures. The results indicated half of the known fake reviews scored well below a similarity value which would flag them as suspicious.

Section \ref{section:aspect-mining} proposes a method to detect opinion spam, using recent research aimed at extracting product aspects from short texts, such as user opinions and forums. In recent years, topic modeling and in particular Latent Dirichlet Allocation (LDA) have been proven to work very well for this problem. LDA can capture the intuition that documents are mixtures of latent semantic topics. The novelty of the proposed method is to use the similarity of the underlying topic distributions of reviews to classify them as truthful or deceptive.

The quality of the topics is known to be low for short and sparse text, without proper preprocessing. I have experimented with different word filtering strategies, besides the standard removal of stopwords, in an attempt to improve the topics quality. I eliminated uninformative, highly seller-specific words as well as highly frequent words. Evaluation of the model results indicated the filtering improved the classifier performance considerably. 
They also showed that combining opinion spam detection with topic modeling can offer, for some number of topics, results in the vicinity of the vectorial-semantic models described in section \ref{section:singleton-detection}. 

The LDA-based model worked best for only a couple of different number of topics, but it performed badly for either a low or a high topics number. Its precision suddenly spiked up when words were more aggressively filtered away, but it did not perform well when a larger number of topics was used. Then it registered significant drops in precision as the threshold was increased.

The work presented in this thesis could be extended in many ways and the following section mentions possible improvements of the detection models and future research directions.

\section{Future research directions}

Several improvements could be made to the semantic measure used in section \ref{section:singleton-detection}. The words \textit{idf} values should be computed for the reviews corpus built from the available dataset. This has not been performed before the semantic measure was computed, the existing \textit{idf} values from the academic toolkit were used. These default values originated from a different corpus in a different context than the sellers reviews. 

WordNet gives very good results when used for word-to-word comparisons, extending the semantic similarity measure to sentence-to-sentence and document-to-document levels should involve more contextual information in order to mitigate the word sense disambiguation problem. In the toolkit I have used, selecting a word's sense is handled in two ways: either the most frequent sense of a word is always used, either for all the senses of a word, the maximum or average of the relatedness score from WordNet is preferred. Selecting the most frequent sense of a word for example may not always be the best choice and this directly impacts the similarity score. 

The clustering step in the first model could be improved considerably by adding new behavioral features, linked to suspicious user activity. These could be engineered for instance from time windows which capture unusual patterns related to a seller's overall rating, such as sudden peaks in the usual number of reviews within a certain period, correlated with an increase in the average rating. The noise generated by the clustering is also directly related to the engineering of the features, as better features produce higher quality clusters. 

Density-based clustering is only one method for reducing the runtime of the detection model. A simpler clustering algorithm, such as k-means could also work well for dividing the workload into reasonable chunks. The number of clusters could be chosen upfront in such a way as to make the number of comparisons acceptable. Then as new reviews are posted, they could be incrementally assigned fast to one of the clusters.  

Recent work on aspect-based opinion mining has shown bag-of-opinion phrases outperform bag-of-words topic models. It would be interesting to see whether running an LDA model on opinion phrases instead of parts-of-speech extracted using a tagger would improve the performance of the review classifier. Intuitively, this approach could pinpoint more subtle deceptive behavior since spammers aim to praise certain aspects of a product or brand.

The two detection models presented in this research have been tested on one dataset so far with very good results. The distributional difference shown by the vectorial and semantic methods on a second dataset has been computed and discussed. Thus the probability that these detection models would work on other real-life datasets looks promising and new research should explore this further.